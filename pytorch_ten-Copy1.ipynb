{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Edit Networks\n",
    "\n",
    "This notebook contains applications of graph edit networks (without edge actions) to trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Boolean Formula Simplification\n",
    "\n",
    "We generate a random Boolean formula over the variables $x$ and $y$ with at most 3 binary operators and then apply simplification rules until none apply anymore. The time series is the series of iteratively simpler formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 10\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 30000\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 4\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 0.818275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-f1f84378ff64>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;31m# sample a nontrivial time series\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0munique\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m             \u001B[0mtime_series\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mboolean_formulae\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_unique_time_series\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_set\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m             \u001B[0mtime_series\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mboolean_formulae\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_time_series\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mX:\\Faks\\DS\\sem4\\Reproducibility\\code\\boolean_formulae.py\u001B[0m in \u001B[0;36mgenerate_unique_time_series\u001B[1;34m(unique_As, generator)\u001B[0m\n\u001B[0;32m    656\u001B[0m     \u001B[0mtrain_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    657\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 658\u001B[1;33m         \u001B[0mtime_series\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    659\u001B[0m         \u001B[1;31m# i counter for the graphs, so we will know which series to add.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    660\u001B[0m         \u001B[0mAs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0ma\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0ma\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtime_series\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mX:\\Faks\\DS\\sem4\\Reproducibility\\code\\boolean_formulae.py\u001B[0m in \u001B[0;36mgenerate_time_series\u001B[1;34m(max_op)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \"\"\"\n\u001B[0;32m     74\u001B[0m     \u001B[1;31m# generate a tree first\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[0mnodes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_generate_tree\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m     \u001B[1;31m# and simplify it\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mX:\\Faks\\DS\\sem4\\Reproducibility\\code\\boolean_formulae.py\u001B[0m in \u001B[0;36m_generate_tree\u001B[1;34m(max_op)\u001B[0m\n\u001B[0;32m    122\u001B[0m         \u001B[1;31m# dependent on the remaining numbr of operations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmax_op\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mp_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp_non_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mp_non_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mmtrand.pyx\u001B[0m in \u001B[0;36mnumpy.random.mtrand.RandomState.choice\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mprod\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\repro\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36mprod\u001B[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[0;32m   3049\u001B[0m     \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3050\u001B[0m     \"\"\"\n\u001B[1;32m-> 3051\u001B[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001B[0m\u001B[0;32m   3052\u001B[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001B[0;32m   3053\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\repro\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36m_wrapreduction\u001B[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_wrapreduction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mufunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m     passkwargs = {k: v for k, v in kwargs.items()\n\u001B[0m\u001B[0;32m     71\u001B[0m                   if v is not np._NoValue}\n\u001B[0;32m     72\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import boolean_formulae\n",
    "\n",
    "unique = True\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "    test_set, unique_As = boolean_formulae.create_test_set(N_test)\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = boolean_formulae.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        if unique:\n",
    "            time_series = boolean_formulae.generate_unique_time_series(test_set)\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = boolean_formulae.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 100 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "\n",
    "    print('Done training')\n",
    "    learning_curves.append(learning_curve)\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        if unique:\n",
    "            time_series = test_set[j]\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = boolean_formulae.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    boolean_formulae.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Addition\n",
    "\n",
    "We generate an addition formula of at most four numbers in the range 1-3 and then use the Peano addition axiom to compute the addition. In particular, the following four rules apply.\n",
    "\n",
    "1. +(m, 0) for any m can be replaced with m.\n",
    "2. +(m, succ(n)) can be replaced with succ(+(m, n)).\n",
    "3. +(m, n) for n in the range 0-9 can be replaced with +(m, succ(n-1)) where -1 refers to the numeric subtraction of 1.\n",
    "4. succ(n) for n in the range 0-9 can be replaced with n+1 (mod 10 because we don't permit two-digit numbers).\n",
    "\n",
    "A time series arises by applying to a current tree every rule that is applicable until a tree results which is only a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 100\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 30000\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "# a single layer with sufficient neurons should suffice here, because we only need to\n",
    "# check immediate parents and children to check whether a rule applies\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 2\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 344.326\n",
      "loss avg after 200 epochs: 231.102\n",
      "loss avg after 300 epochs: 148.784\n",
      "loss avg after 400 epochs: 111.136\n",
      "loss avg after 500 epochs: 78.42\n",
      "loss avg after 600 epochs: 55.3165\n",
      "loss avg after 700 epochs: 38.7933\n",
      "loss avg after 800 epochs: 23.3746\n",
      "loss avg after 900 epochs: 18.987\n",
      "loss avg after 1000 epochs: 13.0233\n",
      "loss avg after 1100 epochs: 10.1341\n",
      "loss avg after 1200 epochs: 5.6443\n",
      "loss avg after 1300 epochs: 4.56359\n",
      "loss avg after 1400 epochs: 4.75965\n",
      "loss avg after 1500 epochs: 2.31081\n",
      "loss avg after 1600 epochs: 2.33182\n",
      "loss avg after 1700 epochs: 1.42122\n",
      "loss avg after 1800 epochs: 1.66433\n",
      "loss avg after 1900 epochs: 1.02555\n",
      "loss avg after 2000 epochs: 1.06445\n",
      "loss avg after 2100 epochs: 0.85327\n",
      "loss avg after 2200 epochs: 0.735521\n",
      "loss avg after 2300 epochs: 0.807941\n",
      "loss avg after 2400 epochs: 0.595343\n",
      "loss avg after 2500 epochs: 0.570323\n",
      "loss avg after 2600 epochs: 0.496682\n",
      "loss avg after 2700 epochs: 0.406751\n",
      "loss avg after 2800 epochs: 1.11963\n",
      "loss avg after 2900 epochs: 0.672426\n",
      "loss avg after 3000 epochs: 0.531198\n",
      "loss avg after 3100 epochs: 0.351914\n",
      "loss avg after 3200 epochs: 0.287452\n",
      "loss avg after 3300 epochs: 0.223392\n",
      "loss avg after 3400 epochs: 0.181713\n",
      "loss avg after 3500 epochs: 0.18589\n",
      "loss avg after 3600 epochs: 0.17348\n",
      "loss avg after 3700 epochs: 0.0964815\n",
      "loss avg after 3800 epochs: 0.134086\n",
      "loss avg after 3900 epochs: 0.10585\n",
      "loss avg after 4000 epochs: 0.115394\n",
      "loss avg after 4100 epochs: 0.106183\n",
      "loss avg after 4200 epochs: 0.0867875\n",
      "loss avg after 4300 epochs: 0.092523\n",
      "loss avg after 4400 epochs: 0.14023\n",
      "loss avg after 4500 epochs: 9.21922\n",
      "loss avg after 4600 epochs: 1.60961\n",
      "loss avg after 4700 epochs: 0.815007\n",
      "loss avg after 4800 epochs: 0.261211\n",
      "loss avg after 4900 epochs: 0.228621\n",
      "loss avg after 5000 epochs: 0.209168\n",
      "loss avg after 5100 epochs: 0.154689\n",
      "loss avg after 5200 epochs: 0.154793\n",
      "loss avg after 5300 epochs: 0.211728\n",
      "loss avg after 5400 epochs: 0.106784\n",
      "loss avg after 5500 epochs: 0.0783545\n",
      "loss avg after 5600 epochs: 0.0684959\n",
      "loss avg after 5700 epochs: 0.0736988\n",
      "loss avg after 5800 epochs: 0.0661418\n",
      "loss avg after 5900 epochs: 0.0449248\n",
      "loss avg after 6000 epochs: 0.0547049\n",
      "loss avg after 6100 epochs: 0.0789384\n",
      "loss avg after 6200 epochs: 0.0569286\n",
      "loss avg after 6300 epochs: 0.0469078\n",
      "loss avg after 6400 epochs: 0.0392681\n",
      "loss avg after 6500 epochs: 0.03442\n",
      "loss avg after 6600 epochs: 0.0376785\n",
      "loss avg after 6700 epochs: 0.028113\n",
      "loss avg after 6800 epochs: 0.034139\n",
      "loss avg after 6900 epochs: 0.0245501\n",
      "loss avg after 7000 epochs: 0.0187433\n",
      "loss avg after 7100 epochs: 0.022456\n",
      "loss avg after 7200 epochs: 0.0184533\n",
      "loss avg after 7300 epochs: 0.0213496\n",
      "loss avg after 7400 epochs: 0.0164504\n",
      "loss avg after 7500 epochs: 0.0184672\n",
      "loss avg after 7600 epochs: 0.0189819\n",
      "loss avg after 7700 epochs: 0.0124556\n",
      "loss avg after 7800 epochs: 0.0206681\n",
      "loss avg after 7900 epochs: 0.0125428\n",
      "loss avg after 8000 epochs: 0.0120371\n",
      "loss avg after 8100 epochs: 0.0116197\n",
      "loss avg after 8200 epochs: 0.0101105\n",
      "loss avg after 8300 epochs: 0.0275957\n",
      "loss avg after 8400 epochs: 0.0369872\n",
      "loss avg after 8500 epochs: 0.013906\n",
      "loss avg after 8600 epochs: 0.011052\n",
      "loss avg after 8700 epochs: 0.00884046\n",
      "loss avg after 8800 epochs: 0.00750884\n",
      "loss avg after 8900 epochs: 0.00742377\n",
      "loss avg after 9000 epochs: 0.00728911\n",
      "loss avg after 9100 epochs: 1.88257\n",
      "loss avg after 9200 epochs: 0.171972\n",
      "loss avg after 9300 epochs: 0.0403933\n",
      "loss avg after 9400 epochs: 0.0319172\n",
      "loss avg after 9500 epochs: 0.0200213\n",
      "loss avg after 9600 epochs: 0.0138675\n",
      "loss avg after 9700 epochs: 0.0165509\n",
      "loss avg after 9800 epochs: 0.0173062\n",
      "loss avg after 9900 epochs: 0.0123086\n",
      "loss avg after 10000 epochs: 0.0110238\n",
      "loss avg after 10100 epochs: 0.00933141\n",
      "loss avg after 10200 epochs: 0.0082381\n",
      "loss avg after 10300 epochs: 0.00688233\n",
      "loss avg after 10400 epochs: 0.00600228\n",
      "loss avg after 10500 epochs: 0.00628615\n",
      "loss avg after 10600 epochs: 0.00511768\n",
      "loss avg after 10700 epochs: 0.00467561\n",
      "loss avg after 10800 epochs: 0.00397222\n",
      "loss avg after 10900 epochs: 0.00423263\n",
      "loss avg after 11000 epochs: 0.00315542\n",
      "loss avg after 11100 epochs: 0.00386033\n",
      "loss avg after 11200 epochs: 0.00402321\n",
      "loss avg after 11300 epochs: 0.00498234\n",
      "loss avg after 11400 epochs: 0.00301643\n",
      "loss avg after 11500 epochs: 0.00270544\n",
      "loss avg after 11600 epochs: 0.00309812\n",
      "loss avg after 11700 epochs: 0.00221596\n",
      "loss avg after 11800 epochs: 0.0022811\n",
      "loss avg after 11900 epochs: 0.00282595\n",
      "loss avg after 12000 epochs: 0.00172979\n",
      "loss avg after 12100 epochs: 0.00189949\n",
      "loss avg after 12200 epochs: 0.00201273\n",
      "loss avg after 12300 epochs: 0.00159774\n",
      "loss avg after 12400 epochs: 0.00166378\n",
      "loss avg after 12500 epochs: 0.0015293\n",
      "loss avg after 12600 epochs: 0.00159618\n",
      "loss avg after 12700 epochs: 0.00132761\n",
      "accuracy: 1\n",
      "--- repeat 2 of 5 ---\n",
      "loss avg after 100 epochs: 310.798\n",
      "loss avg after 200 epochs: 151.028\n",
      "loss avg after 300 epochs: 112.503\n",
      "loss avg after 400 epochs: 74.2534\n",
      "loss avg after 500 epochs: 48.4219\n",
      "loss avg after 600 epochs: 42.5927\n",
      "loss avg after 700 epochs: 26.5561\n",
      "loss avg after 800 epochs: 14.1492\n",
      "loss avg after 900 epochs: 12.8188\n",
      "loss avg after 1000 epochs: 7.58213\n",
      "loss avg after 1100 epochs: 5.99373\n",
      "loss avg after 1200 epochs: 3.50841\n",
      "loss avg after 1300 epochs: 3.69893\n",
      "loss avg after 1400 epochs: 2.6624\n",
      "loss avg after 1500 epochs: 1.5346\n",
      "loss avg after 1600 epochs: 1.48664\n",
      "loss avg after 1700 epochs: 1.50213\n",
      "loss avg after 1800 epochs: 1.10029\n",
      "loss avg after 1900 epochs: 0.954858\n",
      "loss avg after 2000 epochs: 0.870193\n",
      "loss avg after 2100 epochs: 0.576203\n",
      "loss avg after 2200 epochs: 0.472005\n",
      "loss avg after 2300 epochs: 0.416954\n",
      "loss avg after 2400 epochs: 0.460583\n",
      "loss avg after 2500 epochs: 0.325264\n",
      "loss avg after 2600 epochs: 0.308517\n",
      "loss avg after 2700 epochs: 0.58053\n",
      "loss avg after 2800 epochs: 13.1145\n",
      "loss avg after 2900 epochs: 0.585551\n",
      "loss avg after 3000 epochs: 0.464783\n",
      "loss avg after 3100 epochs: 0.244685\n",
      "loss avg after 3200 epochs: 0.244682\n",
      "loss avg after 3300 epochs: 0.241127\n",
      "loss avg after 3400 epochs: 0.311293\n",
      "loss avg after 3500 epochs: 0.182408\n",
      "loss avg after 3600 epochs: 0.169645\n",
      "loss avg after 3700 epochs: 0.169268\n",
      "loss avg after 3800 epochs: 0.127693\n",
      "loss avg after 3900 epochs: 0.10619\n",
      "loss avg after 4000 epochs: 0.118108\n",
      "loss avg after 4100 epochs: 0.137925\n",
      "loss avg after 4200 epochs: 0.0981527\n",
      "loss avg after 4300 epochs: 0.10238\n",
      "loss avg after 4400 epochs: 0.0749587\n",
      "loss avg after 4500 epochs: 0.0712133\n",
      "loss avg after 4600 epochs: 0.0531734\n",
      "loss avg after 4700 epochs: 0.0673551\n",
      "loss avg after 4800 epochs: 0.0561669\n",
      "loss avg after 4900 epochs: 0.0435037\n",
      "loss avg after 5000 epochs: 0.044045\n",
      "loss avg after 5100 epochs: 0.0464443\n",
      "loss avg after 5200 epochs: 0.051281\n",
      "loss avg after 5300 epochs: 0.0443034\n",
      "loss avg after 5400 epochs: 0.028433\n",
      "loss avg after 5500 epochs: 0.0380939\n",
      "loss avg after 5600 epochs: 0.0370996\n",
      "loss avg after 5700 epochs: 0.0303422\n",
      "loss avg after 5800 epochs: 0.0248939\n",
      "loss avg after 5900 epochs: 0.0236239\n",
      "loss avg after 6000 epochs: 0.0218228\n",
      "loss avg after 6100 epochs: 0.0243779\n",
      "loss avg after 6200 epochs: 0.0222285\n",
      "loss avg after 6300 epochs: 0.021225\n",
      "loss avg after 6400 epochs: 0.025302\n",
      "loss avg after 6500 epochs: 0.0194223\n",
      "loss avg after 6600 epochs: 12.5581\n",
      "loss avg after 6700 epochs: 0.159255\n",
      "loss avg after 6800 epochs: 0.0986053\n",
      "loss avg after 6900 epochs: 0.0669907\n",
      "loss avg after 7000 epochs: 0.0847153\n",
      "loss avg after 7100 epochs: 0.048289\n",
      "loss avg after 7200 epochs: 0.0454855\n",
      "loss avg after 7300 epochs: 0.027249\n",
      "loss avg after 7400 epochs: 0.0293706\n",
      "loss avg after 7500 epochs: 0.0230037\n",
      "loss avg after 7600 epochs: 0.0241172\n",
      "loss avg after 7700 epochs: 0.0252976\n",
      "loss avg after 7800 epochs: 0.0140163\n",
      "loss avg after 7900 epochs: 0.0194465\n",
      "loss avg after 8000 epochs: 0.0188556\n",
      "loss avg after 8100 epochs: 0.0119386\n",
      "loss avg after 8200 epochs: 0.0158392\n",
      "loss avg after 8300 epochs: 0.0120324\n",
      "loss avg after 8400 epochs: 0.0116908\n",
      "loss avg after 8500 epochs: 0.0098844\n",
      "loss avg after 8600 epochs: 0.0110967\n",
      "loss avg after 8700 epochs: 0.00762582\n",
      "loss avg after 8800 epochs: 0.00845088\n",
      "loss avg after 8900 epochs: 0.0065273\n",
      "loss avg after 9000 epochs: 0.00641221\n",
      "loss avg after 9100 epochs: 0.00619499\n",
      "loss avg after 9200 epochs: 0.00556994\n",
      "loss avg after 9300 epochs: 0.00500572\n",
      "loss avg after 9400 epochs: 0.00460678\n",
      "loss avg after 9500 epochs: 0.00461894\n",
      "loss avg after 9600 epochs: 0.00597307\n",
      "loss avg after 9700 epochs: 0.00469976\n",
      "loss avg after 9800 epochs: 0.00317023\n",
      "loss avg after 9900 epochs: 0.0019294\n",
      "loss avg after 10000 epochs: 0.00480776\n",
      "loss avg after 10100 epochs: 0.00284584\n",
      "loss avg after 10200 epochs: 0.00321502\n",
      "loss avg after 10300 epochs: 0.00329126\n",
      "loss avg after 10400 epochs: 0.00412039\n",
      "loss avg after 10500 epochs: 0.00339731\n",
      "loss avg after 10600 epochs: 0.00321823\n",
      "loss avg after 10700 epochs: 0.00277723\n",
      "loss avg after 10800 epochs: 0.00409338\n",
      "loss avg after 10900 epochs: 0.00245808\n",
      "loss avg after 11000 epochs: 0.00156765\n",
      "loss avg after 11100 epochs: 0.00168727\n",
      "loss avg after 11200 epochs: 0.00134831\n",
      "loss avg after 11300 epochs: 0.00128636\n",
      "loss avg after 11400 epochs: 0.0013574\n",
      "accuracy: 1\n",
      "--- repeat 3 of 5 ---\n",
      "loss avg after 100 epochs: 354.186\n",
      "loss avg after 200 epochs: 238.64\n",
      "loss avg after 300 epochs: 129.4\n",
      "loss avg after 400 epochs: 76.3558\n",
      "loss avg after 500 epochs: 59.0821\n",
      "loss avg after 600 epochs: 40.8819\n",
      "loss avg after 700 epochs: 29.3995\n",
      "loss avg after 800 epochs: 20.9662\n",
      "loss avg after 900 epochs: 12.0587\n",
      "loss avg after 1000 epochs: 8.98986\n",
      "loss avg after 1100 epochs: 6.4464\n",
      "loss avg after 1200 epochs: 5.5606\n",
      "loss avg after 1300 epochs: 3.58486\n",
      "loss avg after 1400 epochs: 2.3858\n",
      "loss avg after 1500 epochs: 1.51069\n",
      "loss avg after 1600 epochs: 1.76868\n",
      "loss avg after 1700 epochs: 1.23726\n",
      "loss avg after 1800 epochs: 1.13292\n",
      "loss avg after 1900 epochs: 0.782572\n",
      "loss avg after 2000 epochs: 0.773183\n",
      "loss avg after 2100 epochs: 0.573065\n",
      "loss avg after 2200 epochs: 0.550545\n",
      "loss avg after 2300 epochs: 0.616472\n",
      "loss avg after 2400 epochs: 0.404402\n",
      "loss avg after 2500 epochs: 0.299991\n",
      "loss avg after 2600 epochs: 0.201002\n",
      "loss avg after 2700 epochs: 0.453235\n",
      "loss avg after 2800 epochs: 0.280339\n",
      "loss avg after 2900 epochs: 0.222548\n",
      "loss avg after 3000 epochs: 0.6224\n",
      "loss avg after 3100 epochs: 0.234998\n",
      "loss avg after 3200 epochs: 0.177551\n",
      "loss avg after 3300 epochs: 2.47693\n",
      "loss avg after 3400 epochs: 0.492361\n",
      "loss avg after 3500 epochs: 0.173111\n",
      "loss avg after 3600 epochs: 0.184182\n",
      "loss avg after 3700 epochs: 0.178944\n",
      "loss avg after 3800 epochs: 0.159454\n",
      "loss avg after 3900 epochs: 0.136319\n",
      "loss avg after 4000 epochs: 0.12581\n",
      "loss avg after 4100 epochs: 0.118364\n",
      "loss avg after 4200 epochs: 0.0799935\n",
      "loss avg after 4300 epochs: 0.08704\n",
      "loss avg after 4400 epochs: 0.0939398\n",
      "loss avg after 4500 epochs: 0.0555381\n",
      "loss avg after 4600 epochs: 0.133671\n",
      "loss avg after 4700 epochs: 0.0623898\n",
      "loss avg after 4800 epochs: 0.0972131\n",
      "loss avg after 4900 epochs: 0.0379378\n",
      "loss avg after 5000 epochs: 0.0467855\n",
      "loss avg after 5100 epochs: 0.0667596\n",
      "loss avg after 5200 epochs: 0.0334988\n",
      "loss avg after 5300 epochs: 0.0434253\n",
      "loss avg after 5400 epochs: 0.0565086\n",
      "loss avg after 5500 epochs: 0.0340791\n",
      "loss avg after 5600 epochs: 0.0369723\n",
      "loss avg after 5700 epochs: 0.0305995\n",
      "loss avg after 5800 epochs: 0.0253922\n",
      "loss avg after 5900 epochs: 0.0223573\n",
      "loss avg after 6000 epochs: 0.0266883\n",
      "loss avg after 6100 epochs: 0.398203\n",
      "loss avg after 6200 epochs: 1.59981\n",
      "loss avg after 6300 epochs: 0.152275\n",
      "loss avg after 6400 epochs: 0.131572\n",
      "loss avg after 6500 epochs: 0.0668368\n",
      "loss avg after 6600 epochs: 0.0493704\n",
      "loss avg after 6700 epochs: 0.042183\n",
      "loss avg after 6800 epochs: 0.0647756\n",
      "loss avg after 6900 epochs: 0.0358079\n",
      "loss avg after 7000 epochs: 0.0253769\n",
      "loss avg after 7100 epochs: 0.02151\n",
      "loss avg after 7200 epochs: 0.0157128\n",
      "loss avg after 7300 epochs: 0.0146984\n",
      "loss avg after 7400 epochs: 0.0169605\n",
      "loss avg after 7500 epochs: 0.0156545\n",
      "loss avg after 7600 epochs: 0.0158988\n",
      "loss avg after 7700 epochs: 0.0105279\n",
      "loss avg after 7800 epochs: 0.0108368\n",
      "loss avg after 7900 epochs: 0.0127776\n",
      "loss avg after 8000 epochs: 0.00932265\n",
      "loss avg after 8100 epochs: 0.00942678\n",
      "loss avg after 8200 epochs: 0.00854332\n",
      "loss avg after 8300 epochs: 0.00711828\n",
      "loss avg after 8400 epochs: 0.00827067\n",
      "loss avg after 8500 epochs: 0.00663931\n",
      "loss avg after 8600 epochs: 0.00560325\n",
      "loss avg after 8700 epochs: 0.00518442\n",
      "loss avg after 8800 epochs: 0.00479382\n",
      "loss avg after 8900 epochs: 0.00697699\n",
      "loss avg after 9000 epochs: 0.00548195\n",
      "loss avg after 9100 epochs: 0.00405687\n",
      "loss avg after 9200 epochs: 0.00395537\n",
      "loss avg after 9300 epochs: 0.00368174\n",
      "loss avg after 9400 epochs: 0.00463081\n",
      "loss avg after 9500 epochs: 0.00664259\n",
      "loss avg after 9600 epochs: 0.00380919\n",
      "loss avg after 9700 epochs: 0.0040909\n",
      "loss avg after 9800 epochs: 0.00379964\n",
      "loss avg after 9900 epochs: 0.00307776\n",
      "loss avg after 10000 epochs: 0.00219096\n",
      "loss avg after 10100 epochs: 0.00393831\n",
      "loss avg after 10200 epochs: 0.00386361\n",
      "loss avg after 10300 epochs: 0.00224099\n",
      "loss avg after 10400 epochs: 0.00752649\n",
      "loss avg after 10500 epochs: 0.00288003\n",
      "loss avg after 10600 epochs: 0.00358374\n",
      "loss avg after 10700 epochs: 0.00235623\n",
      "loss avg after 10800 epochs: 0.00270249\n",
      "loss avg after 10900 epochs: 0.00144524\n",
      "loss avg after 11000 epochs: 0.0017665\n",
      "loss avg after 11100 epochs: 0.00161086\n",
      "loss avg after 11200 epochs: 0.00237067\n",
      "loss avg after 11300 epochs: 0.00127838\n",
      "loss avg after 11400 epochs: 0.00133334\n",
      "accuracy: 1\n",
      "--- repeat 4 of 5 ---\n",
      "loss avg after 100 epochs: 284.814\n",
      "loss avg after 200 epochs: 141.919\n",
      "loss avg after 300 epochs: 98.2708\n",
      "loss avg after 400 epochs: 68.7998\n",
      "loss avg after 500 epochs: 40.7443\n",
      "loss avg after 600 epochs: 22.9458\n",
      "loss avg after 700 epochs: 20.8654\n",
      "loss avg after 800 epochs: 10.7143\n",
      "loss avg after 900 epochs: 6.39821\n",
      "loss avg after 1000 epochs: 5.06681\n",
      "loss avg after 1100 epochs: 2.83172\n",
      "loss avg after 1200 epochs: 3.65058\n",
      "loss avg after 1300 epochs: 2.82873\n",
      "loss avg after 1400 epochs: 2.6688\n",
      "loss avg after 1500 epochs: 1.40638\n",
      "loss avg after 1600 epochs: 11.7054\n",
      "loss avg after 1700 epochs: 1.24844\n",
      "loss avg after 1800 epochs: 1.60427\n",
      "loss avg after 1900 epochs: 0.93283\n",
      "loss avg after 2000 epochs: 0.735008\n",
      "loss avg after 2100 epochs: 0.467969\n",
      "loss avg after 2200 epochs: 1.04976\n",
      "loss avg after 2300 epochs: 0.439212\n",
      "loss avg after 2400 epochs: 0.473839\n",
      "loss avg after 2500 epochs: 0.444729\n",
      "loss avg after 2600 epochs: 0.416681\n",
      "loss avg after 2700 epochs: 0.256073\n",
      "loss avg after 2800 epochs: 0.325077\n",
      "loss avg after 2900 epochs: 0.152537\n",
      "loss avg after 3000 epochs: 0.273377\n",
      "loss avg after 3100 epochs: 0.327556\n",
      "loss avg after 3200 epochs: 0.2052\n",
      "loss avg after 3300 epochs: 0.146751\n",
      "loss avg after 3400 epochs: 0.15052\n",
      "loss avg after 3500 epochs: 0.1895\n",
      "loss avg after 3600 epochs: 0.118687\n",
      "loss avg after 3700 epochs: 0.126697\n",
      "loss avg after 3800 epochs: 0.142754\n",
      "loss avg after 3900 epochs: 0.106283\n",
      "loss avg after 4000 epochs: 0.0697436\n",
      "loss avg after 4100 epochs: 0.122966\n",
      "loss avg after 4200 epochs: 0.106425\n",
      "loss avg after 4300 epochs: 0.0902656\n",
      "loss avg after 4400 epochs: 0.0983763\n",
      "loss avg after 4500 epochs: 0.0696483\n",
      "loss avg after 4600 epochs: 0.0717989\n",
      "loss avg after 4700 epochs: 2.80394\n",
      "loss avg after 4800 epochs: 0.152102\n",
      "loss avg after 4900 epochs: 0.125499\n",
      "loss avg after 5000 epochs: 0.211487\n",
      "loss avg after 5100 epochs: 0.0606976\n",
      "loss avg after 5200 epochs: 0.0608507\n",
      "loss avg after 5300 epochs: 0.0468717\n",
      "loss avg after 5400 epochs: 0.0680112\n",
      "loss avg after 5500 epochs: 0.0411396\n",
      "loss avg after 5600 epochs: 0.040517\n",
      "loss avg after 5700 epochs: 0.0317616\n",
      "loss avg after 5800 epochs: 0.0304629\n",
      "loss avg after 5900 epochs: 0.0349576\n",
      "loss avg after 6000 epochs: 0.0299608\n",
      "loss avg after 6100 epochs: 0.0216719\n",
      "loss avg after 6200 epochs: 0.0284556\n",
      "loss avg after 6300 epochs: 0.0263742\n",
      "loss avg after 6400 epochs: 0.0196243\n",
      "loss avg after 6500 epochs: 0.0216359\n",
      "loss avg after 6600 epochs: 0.0169164\n",
      "loss avg after 6700 epochs: 0.0189237\n",
      "loss avg after 6800 epochs: 0.0144781\n",
      "loss avg after 6900 epochs: 0.0177614\n",
      "loss avg after 7000 epochs: 0.0135509\n",
      "loss avg after 7100 epochs: 0.013213\n",
      "loss avg after 7200 epochs: 0.0108398\n",
      "loss avg after 7300 epochs: 0.0128092\n",
      "loss avg after 7400 epochs: 0.0122798\n",
      "loss avg after 7500 epochs: 0.0124625\n",
      "loss avg after 7600 epochs: 0.0112332\n",
      "loss avg after 7700 epochs: 0.00847663\n",
      "loss avg after 7800 epochs: 0.007159\n",
      "loss avg after 7900 epochs: 0.0103998\n",
      "loss avg after 8000 epochs: 0.00878018\n",
      "loss avg after 8100 epochs: 0.0085399\n",
      "loss avg after 8200 epochs: 0.153185\n",
      "loss avg after 8300 epochs: 0.0576015\n",
      "loss avg after 8400 epochs: 0.043369\n",
      "loss avg after 8500 epochs: 0.028557\n",
      "loss avg after 8600 epochs: 0.0229072\n",
      "loss avg after 8700 epochs: 0.0241942\n",
      "loss avg after 8800 epochs: 0.0155137\n",
      "loss avg after 8900 epochs: 0.0162734\n",
      "loss avg after 9000 epochs: 0.0115823\n",
      "loss avg after 9100 epochs: 0.0123451\n",
      "loss avg after 9200 epochs: 0.0152289\n",
      "loss avg after 9300 epochs: 0.0101877\n",
      "loss avg after 9400 epochs: 0.00900372\n",
      "loss avg after 9500 epochs: 0.00873146\n",
      "loss avg after 9600 epochs: 0.00721168\n",
      "loss avg after 9700 epochs: 0.00716425\n",
      "loss avg after 9800 epochs: 0.00510717\n",
      "loss avg after 9900 epochs: 0.00518483\n",
      "loss avg after 10000 epochs: 0.00418214\n",
      "loss avg after 10100 epochs: 0.0043608\n",
      "loss avg after 10200 epochs: 0.00446416\n",
      "loss avg after 10300 epochs: 0.00403783\n",
      "loss avg after 10400 epochs: 0.00384257\n",
      "loss avg after 10500 epochs: 0.00305643\n",
      "loss avg after 10600 epochs: 0.00448495\n",
      "loss avg after 10700 epochs: 0.00872217\n",
      "loss avg after 10800 epochs: 0.00521286\n",
      "loss avg after 10900 epochs: 0.00408609\n",
      "loss avg after 11000 epochs: 0.0039945\n",
      "loss avg after 11100 epochs: 0.00291486\n",
      "loss avg after 11200 epochs: 0.00281053\n",
      "loss avg after 11300 epochs: 0.00321164\n",
      "loss avg after 11400 epochs: 0.00272961\n",
      "loss avg after 11500 epochs: 0.00205679\n",
      "loss avg after 11600 epochs: 0.00205193\n",
      "loss avg after 11700 epochs: 0.00155186\n",
      "loss avg after 11800 epochs: 0.00194757\n",
      "loss avg after 11900 epochs: 0.00160772\n",
      "loss avg after 12000 epochs: 0.00132379\n",
      "loss avg after 12100 epochs: 0.00146582\n",
      "accuracy: 1\n",
      "--- repeat 5 of 5 ---\n",
      "loss avg after 100 epochs: 326.378\n",
      "loss avg after 200 epochs: 169.619\n",
      "loss avg after 300 epochs: 116.868\n",
      "loss avg after 400 epochs: 103.138\n",
      "loss avg after 500 epochs: 62.6953\n",
      "loss avg after 600 epochs: 44.9762\n",
      "loss avg after 700 epochs: 34.4823\n",
      "loss avg after 800 epochs: 20.5367\n",
      "loss avg after 900 epochs: 14.0947\n",
      "loss avg after 1000 epochs: 7.10877\n",
      "loss avg after 1100 epochs: 4.85167\n",
      "loss avg after 1200 epochs: 5.03172\n",
      "loss avg after 1300 epochs: 20.0768\n",
      "loss avg after 1400 epochs: 3.82138\n",
      "loss avg after 1500 epochs: 2.17948\n",
      "loss avg after 1600 epochs: 1.75237\n",
      "loss avg after 1700 epochs: 1.2749\n",
      "loss avg after 1800 epochs: 1.2798\n",
      "loss avg after 1900 epochs: 0.828483\n",
      "loss avg after 2000 epochs: 0.717558\n",
      "loss avg after 2100 epochs: 0.701805\n",
      "loss avg after 2200 epochs: 0.642531\n",
      "loss avg after 2300 epochs: 0.512384\n",
      "loss avg after 2400 epochs: 0.465397\n",
      "loss avg after 2500 epochs: 0.452186\n",
      "loss avg after 2600 epochs: 0.254106\n",
      "loss avg after 2700 epochs: 0.520097\n",
      "loss avg after 2800 epochs: 0.212904\n",
      "loss avg after 2900 epochs: 0.238883\n",
      "loss avg after 3000 epochs: 0.197089\n",
      "loss avg after 3100 epochs: 0.190945\n",
      "loss avg after 3200 epochs: 0.127312\n",
      "loss avg after 3300 epochs: 0.186811\n",
      "loss avg after 3400 epochs: 0.179285\n",
      "loss avg after 3500 epochs: 0.144157\n",
      "loss avg after 3600 epochs: 0.120371\n",
      "loss avg after 3700 epochs: 0.112543\n",
      "loss avg after 3800 epochs: 0.0691836\n",
      "loss avg after 3900 epochs: 0.100349\n",
      "loss avg after 4000 epochs: 0.0884121\n",
      "loss avg after 4100 epochs: 0.0665415\n",
      "loss avg after 4200 epochs: 0.0465195\n",
      "loss avg after 4300 epochs: 0.0695656\n",
      "loss avg after 4400 epochs: 0.0651033\n",
      "loss avg after 4500 epochs: 0.0599084\n",
      "loss avg after 4600 epochs: 0.050693\n",
      "loss avg after 4700 epochs: 0.0423662\n",
      "loss avg after 4800 epochs: 0.0339588\n",
      "loss avg after 4900 epochs: 0.0378362\n",
      "loss avg after 5000 epochs: 0.0592484\n",
      "loss avg after 5100 epochs: 0.0274233\n",
      "loss avg after 5200 epochs: 0.0376774\n",
      "loss avg after 5300 epochs: 0.0311841\n",
      "loss avg after 5400 epochs: 0.0284915\n",
      "loss avg after 5500 epochs: 0.0408541\n",
      "loss avg after 5600 epochs: 3.54738\n",
      "loss avg after 5700 epochs: 0.251472\n",
      "loss avg after 5800 epochs: 0.113524\n",
      "loss avg after 5900 epochs: 0.0839714\n",
      "loss avg after 6000 epochs: 0.0617233\n",
      "loss avg after 6100 epochs: 0.0446489\n",
      "loss avg after 6200 epochs: 0.0499261\n",
      "loss avg after 6300 epochs: 0.0476905\n",
      "loss avg after 6400 epochs: 0.0364539\n",
      "loss avg after 6500 epochs: 0.0355999\n",
      "loss avg after 6600 epochs: 0.0353616\n",
      "loss avg after 6700 epochs: 0.0294229\n",
      "loss avg after 6800 epochs: 0.0208816\n",
      "loss avg after 6900 epochs: 0.0248475\n",
      "loss avg after 7000 epochs: 0.0196401\n",
      "loss avg after 7100 epochs: 0.00984076\n",
      "loss avg after 7200 epochs: 0.013881\n",
      "loss avg after 7300 epochs: 0.0108329\n",
      "loss avg after 7400 epochs: 0.042932\n",
      "loss avg after 7500 epochs: 0.0144877\n",
      "loss avg after 7600 epochs: 0.0140842\n",
      "loss avg after 7700 epochs: 0.0147361\n",
      "loss avg after 7800 epochs: 0.00808795\n",
      "loss avg after 7900 epochs: 0.00916767\n",
      "loss avg after 8000 epochs: 0.00962392\n",
      "loss avg after 8100 epochs: 0.012967\n",
      "loss avg after 8200 epochs: 0.00803666\n",
      "loss avg after 8300 epochs: 0.00550792\n",
      "loss avg after 8400 epochs: 0.00497\n",
      "loss avg after 8500 epochs: 0.00715418\n",
      "loss avg after 8600 epochs: 0.00661711\n",
      "loss avg after 8700 epochs: 0.00597213\n",
      "loss avg after 8800 epochs: 0.00374983\n",
      "loss avg after 8900 epochs: 0.00980149\n",
      "loss avg after 9000 epochs: 0.00594438\n",
      "loss avg after 9100 epochs: 0.00521694\n",
      "loss avg after 9200 epochs: 0.00447872\n",
      "loss avg after 9300 epochs: 0.00310178\n",
      "loss avg after 9400 epochs: 0.00285003\n",
      "loss avg after 9500 epochs: 0.00359906\n",
      "loss avg after 9600 epochs: 0.00302495\n",
      "loss avg after 9700 epochs: 0.00295082\n",
      "loss avg after 9800 epochs: 0.00272956\n",
      "loss avg after 9900 epochs: 0.00246637\n",
      "loss avg after 10000 epochs: 0.00236162\n",
      "loss avg after 10100 epochs: 0.00262461\n",
      "loss avg after 10200 epochs: 0.00232227\n",
      "loss avg after 10300 epochs: 0.0100267\n",
      "loss avg after 10400 epochs: 0.0264101\n",
      "loss avg after 10500 epochs: 0.00417656\n",
      "loss avg after 10600 epochs: 0.00285557\n",
      "loss avg after 10700 epochs: 0.00174355\n",
      "loss avg after 10800 epochs: 0.0297492\n",
      "loss avg after 10900 epochs: 0.00209816\n",
      "loss avg after 11000 epochs: 0.00168776\n",
      "loss avg after 11100 epochs: 0.00183893\n",
      "loss avg after 11200 epochs: 0.0111917\n",
      "loss avg after 11300 epochs: 0.00414134\n",
      "loss avg after 11400 epochs: 0.239751\n",
      "loss avg after 11500 epochs: 0.0156145\n",
      "loss avg after 11600 epochs: 0.0104955\n",
      "loss avg after 11700 epochs: 0.00806254\n",
      "loss avg after 11800 epochs: 0.00418798\n",
      "loss avg after 11900 epochs: 0.00444132\n",
      "loss avg after 12000 epochs: 0.00326369\n",
      "loss avg after 12100 epochs: 0.00328224\n",
      "loss avg after 12200 epochs: 0.00184406\n",
      "loss avg after 12300 epochs: 0.00180513\n",
      "loss avg after 12400 epochs: 0.00140966\n",
      "loss avg after 12500 epochs: 0.0016614\n",
      "loss avg after 12600 epochs: 0.00147449\n",
      "loss avg after 12700 epochs: 0.00127409\n",
      "accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import peano_addition\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = peano_addition.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = peano_addition.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 100 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "    learning_curves.append(learning_curve)\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = peano_addition.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    peano_addition.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "repro",
   "language": "python",
   "display_name": "repro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}