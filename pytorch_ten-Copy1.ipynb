{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Edit Networks\n",
    "\n",
    "This notebook contains applications of graph edit networks (without edge actions) to trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Boolean Formula Simplification\n",
    "\n",
    "We generate a random Boolean formula over the variables $x$ and $y$ with at most 3 binary operators and then apply simplification rules until none apply anymore. The time series is the series of iteratively simpler formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 10\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 30000\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 4\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 1.07716\n",
      "loss avg after 200 epochs: 0.0622922\n",
      "loss avg after 300 epochs: 0.0112627\n",
      "loss avg after 400 epochs: 0.00428845\n",
      "loss avg after 500 epochs: 0.00191394\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-a2865215ed2c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[0mtime_series\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mboolean_formulae\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_time_series\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_series\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m             \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m         \u001B[1;31m# iterate over the time series\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_series\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import boolean_formulae\n",
    "\n",
    "unique = True\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "    test_set, unique_As = boolean_formulae.create_test_set(N_test)\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = boolean_formulae.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        if unique:\n",
    "            time_series = boolean_formulae.generate_unique_time_series(test_set)\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = boolean_formulae.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 100 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "    learning_curves.append(learning_curve)\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        if unique:\n",
    "            time_series = test_set[j]\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = boolean_formulae.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    boolean_formulae.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Addition\n",
    "\n",
    "We generate an addition formula of at most four numbers in the range 1-3 and then use the Peano addition axiom to compute the addition. In particular, the following four rules apply.\n",
    "\n",
    "1. +(m, 0) for any m can be replaced with m.\n",
    "2. +(m, succ(n)) can be replaced with succ(+(m, n)).\n",
    "3. +(m, n) for n in the range 0-9 can be replaced with +(m, succ(n-1)) where -1 refers to the numeric subtraction of 1.\n",
    "4. succ(n) for n in the range 0-9 can be replaced with n+1 (mod 10 because we don't permit two-digit numbers).\n",
    "\n",
    "A time series arises by applying to a current tree every rule that is applicable until a tree results which is only a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 100\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 30000\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "# a single layer with sufficient neurons should suffice here, because we only need to\n",
    "# check immediate parents and children to check whether a rule applies\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 2\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 344.326\n",
      "loss avg after 200 epochs: 231.102\n",
      "loss avg after 300 epochs: 148.784\n",
      "loss avg after 400 epochs: 111.136\n",
      "loss avg after 500 epochs: 78.42\n",
      "loss avg after 600 epochs: 55.3165\n",
      "loss avg after 700 epochs: 38.7933\n",
      "loss avg after 800 epochs: 23.3746\n",
      "loss avg after 900 epochs: 18.987\n",
      "loss avg after 1000 epochs: 13.0233\n",
      "loss avg after 1100 epochs: 10.1341\n",
      "loss avg after 1200 epochs: 5.6443\n",
      "loss avg after 1300 epochs: 4.56359\n",
      "loss avg after 1400 epochs: 4.75965\n",
      "loss avg after 1500 epochs: 2.31081\n",
      "loss avg after 1600 epochs: 2.33182\n",
      "loss avg after 1700 epochs: 1.42122\n",
      "loss avg after 1800 epochs: 1.66433\n",
      "loss avg after 1900 epochs: 1.02555\n",
      "loss avg after 2000 epochs: 1.06445\n",
      "loss avg after 2100 epochs: 0.85327\n",
      "loss avg after 2200 epochs: 0.735521\n",
      "loss avg after 2300 epochs: 0.807941\n",
      "loss avg after 2400 epochs: 0.595343\n",
      "loss avg after 2500 epochs: 0.570323\n",
      "loss avg after 2600 epochs: 0.496682\n",
      "loss avg after 2700 epochs: 0.406751\n",
      "loss avg after 2800 epochs: 1.11963\n",
      "loss avg after 2900 epochs: 0.672426\n",
      "loss avg after 3000 epochs: 0.531198\n",
      "loss avg after 3100 epochs: 0.351914\n",
      "loss avg after 3200 epochs: 0.287452\n",
      "loss avg after 3300 epochs: 0.223392\n",
      "loss avg after 3400 epochs: 0.181713\n",
      "loss avg after 3500 epochs: 0.18589\n",
      "loss avg after 3600 epochs: 0.17348\n",
      "loss avg after 3700 epochs: 0.0964815\n",
      "loss avg after 3800 epochs: 0.134086\n",
      "loss avg after 3900 epochs: 0.10585\n",
      "loss avg after 4000 epochs: 0.115394\n",
      "loss avg after 4100 epochs: 0.106183\n",
      "loss avg after 4200 epochs: 0.0867875\n",
      "loss avg after 4300 epochs: 0.092523\n",
      "loss avg after 4400 epochs: 0.14023\n",
      "loss avg after 4500 epochs: 9.21922\n",
      "loss avg after 4600 epochs: 1.60961\n",
      "loss avg after 4700 epochs: 0.815007\n",
      "loss avg after 4800 epochs: 0.261211\n",
      "loss avg after 4900 epochs: 0.228621\n",
      "loss avg after 5000 epochs: 0.209168\n",
      "loss avg after 5100 epochs: 0.154689\n",
      "loss avg after 5200 epochs: 0.154793\n",
      "loss avg after 5300 epochs: 0.211728\n",
      "loss avg after 5400 epochs: 0.106784\n",
      "loss avg after 5500 epochs: 0.0783545\n",
      "loss avg after 5600 epochs: 0.0684959\n",
      "loss avg after 5700 epochs: 0.0736988\n",
      "loss avg after 5800 epochs: 0.0661418\n",
      "loss avg after 5900 epochs: 0.0449248\n",
      "loss avg after 6000 epochs: 0.0547049\n",
      "loss avg after 6100 epochs: 0.0789384\n",
      "loss avg after 6200 epochs: 0.0569286\n",
      "loss avg after 6300 epochs: 0.0469078\n",
      "loss avg after 6400 epochs: 0.0392681\n",
      "loss avg after 6500 epochs: 0.03442\n",
      "loss avg after 6600 epochs: 0.0376785\n",
      "loss avg after 6700 epochs: 0.028113\n",
      "loss avg after 6800 epochs: 0.034139\n",
      "loss avg after 6900 epochs: 0.0245501\n",
      "loss avg after 7000 epochs: 0.0187433\n",
      "loss avg after 7100 epochs: 0.022456\n",
      "loss avg after 7200 epochs: 0.0184533\n",
      "loss avg after 7300 epochs: 0.0213496\n",
      "loss avg after 7400 epochs: 0.0164504\n",
      "loss avg after 7500 epochs: 0.0184672\n",
      "loss avg after 7600 epochs: 0.0189819\n",
      "loss avg after 7700 epochs: 0.0124556\n",
      "loss avg after 7800 epochs: 0.0206681\n",
      "loss avg after 7900 epochs: 0.0125428\n",
      "loss avg after 8000 epochs: 0.0120371\n",
      "loss avg after 8100 epochs: 0.0116197\n",
      "loss avg after 8200 epochs: 0.0101105\n",
      "loss avg after 8300 epochs: 0.0275957\n",
      "loss avg after 8400 epochs: 0.0369872\n",
      "loss avg after 8500 epochs: 0.013906\n",
      "loss avg after 8600 epochs: 0.011052\n",
      "loss avg after 8700 epochs: 0.00884046\n",
      "loss avg after 8800 epochs: 0.00750884\n",
      "loss avg after 8900 epochs: 0.00742377\n",
      "loss avg after 9000 epochs: 0.00728911\n",
      "loss avg after 9100 epochs: 1.88257\n",
      "loss avg after 9200 epochs: 0.171972\n",
      "loss avg after 9300 epochs: 0.0403933\n",
      "loss avg after 9400 epochs: 0.0319172\n",
      "loss avg after 9500 epochs: 0.0200213\n",
      "loss avg after 9600 epochs: 0.0138675\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import peano_addition\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = peano_addition.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = peano_addition.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 100 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "    learning_curves.append(learning_curve)\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = peano_addition.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    peano_addition.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "repro",
   "language": "python",
   "display_name": "repro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}